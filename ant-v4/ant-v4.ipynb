{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     15\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(ENVIRONMENT, render_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     agent \u001b[38;5;241m=\u001b[39m \u001b[43mDQN\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SAVED_MODEL \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     19\u001b[0m         agent\u001b[38;5;241m.\u001b[39mload(SAVED_MODEL)\n",
      "File \u001b[0;32m~/workspace/master-thesis/ant-v4/../models/dqn.py:34\u001b[0m, in \u001b[0;36mDQN.__init__\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplayBuffer \u001b[38;5;241m=\u001b[39m deque(maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Main model\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Target model for future Q values.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# More stable approach.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargetModel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_network()\n",
      "File \u001b[0;32m~/workspace/master-thesis/ant-v4/../models/dqn.py:51\u001b[0m, in \u001b[0;36mDQN.build_network\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m48\u001b[39m, activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     50\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn, activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 51\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[43madam_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from models.dqn import DQN\n",
    "\n",
    "EPOCHS = 500\n",
    "STEPS = 200\n",
    "ENVIRONMENT = \"MountainCar-v0\"\n",
    "EPOCHS_BEFORE_RENDER = 0\n",
    "SAVED_MODEL = \"\"\n",
    "SAVE = True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(ENVIRONMENT, render_mode = \"rgb_array\")\n",
    "    agent = DQN(env=env)\n",
    "\n",
    "    if SAVED_MODEL != \"\":\n",
    "        agent.load(SAVED_MODEL)\n",
    "\n",
    "    # How many challenges we have solved\n",
    "    total_solved = 0\n",
    "    for games in range(EPOCHS):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Start rendering after 400 games.\n",
    "        if games > EPOCHS_BEFORE_RENDER:\n",
    "            if SAVE:\n",
    "                agent.save(\"mountaincar-\" + str(games))\n",
    "                print(\"Saved model\")\n",
    "                SAVE = False\n",
    "            env.render()\n",
    "\n",
    "        for i in range(STEPS):\n",
    "            action = agent.predict_action(state)\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Give reward if X >= 0.5 \n",
    "            # for faster learning.\n",
    "            if new_state[0] >= 0.5:\n",
    "                reward += 1\n",
    "\n",
    "            # Store replay\n",
    "            agent.save_replay(state, action, reward, new_state, done)\n",
    "            # Train agent\n",
    "            agent.train()\n",
    "\n",
    "            total_reward += reward\n",
    "            state = new_state\n",
    "\n",
    "            if games > EPOCHS_BEFORE_RENDER:\n",
    "                env.render()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # If we finish the episode in less than max steps.\n",
    "        if i < 199:\n",
    "            total_solved += 1\n",
    "            print(f\"Game {games + 1} Solved. Total solved: {total_solved}\")\n",
    "        else:\n",
    "            print(f\"Game {games + 1} not solved.\")\n",
    "\n",
    "        # Update network weights.\n",
    "        agent.update_weights()\n",
    "\n",
    "\n",
    "    print(f\"Total solved: {total_solved}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
